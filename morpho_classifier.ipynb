{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "import numpy\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Retrieve example pairs from lexique (-euse, -rice)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "#retrieve our examples\n",
    "df = pandas.read_excel(\"Lexique-query-2023-04-11 13-58-53.xlsx\")\n",
    "def find_class(word):\n",
    "    return int(\"euse\" in word)\n",
    "def get_feminine_word(neutral_lemma, examples):\n",
    "    feminine = \"\"\n",
    "    for lemma, word in examples:\n",
    "        if lemma==neutral_lemma:\n",
    "            feminine = word\n",
    "    return feminine\n",
    "examples = list(zip(list(df[\"lemme\"]), list(df[\"Word\"])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading vectors from our own implementation (2.5m words) and (500m words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#load our own vectors\n",
    "def vec(data, word_to_idx, word):\n",
    "    return numpy.array(data[word_to_idx[word]], dtype=numpy.float32)\n",
    "\n",
    "#load FRCOWS(2.5m) vectors\n",
    "own_model_thin = torch.load(\"./model106.pth\", map_location=torch.device('cpu'))\n",
    "idx_to_word_thin = own_model_thin[\"idx_to_word\"]\n",
    "word_to_idx_thin = own_model_thin[\"word_to_idx\"]\n",
    "data_thin = own_model_thin[\"cbow_state_dict\"]\n",
    "data_thin = data_thin[\"embeddings.weight\"].data\n",
    "own_examples_thin = [(vec(data_thin, word_to_idx_thin, lemme), find_class(word)) for lemme, word in examples if lemme in idx_to_word_thin]\n",
    "\n",
    "#load FRCOWS(500m) vectors\n",
    "own_model_thick = torch.load(\"./model4.pth\", map_location=torch.device('cpu'))\n",
    "idx_to_word_thick = own_model_thick[\"idx_to_word\"]\n",
    "data_thick = own_model_thick[\"cbow_state_dict\"]\n",
    "word_to_idx_thick = own_model_thick[\"word_to_idx\"]\n",
    "data_thick = data_thick[\"embeddings.weight\"].data\n",
    "own_examples_thick = [(vec(data_thick, word_to_idx_thick, lemme), find_class(word)) for lemme, word in examples if lemme in idx_to_word_thick]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading FRCOW(8.8bn words) vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#load FRCOWS(8.8bn) vectors\n",
    "def load_frcows(path):\n",
    "    embeddings={}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for e in lines:\n",
    "            token = e.split(\" \")[0]\n",
    "            # token = token[0:token.rfind(\"_\")]\n",
    "            embedding = numpy.array(e.split(\" \")[1:], dtype=numpy.float32)\n",
    "            embeddings[token] = embedding\n",
    "    return embeddings\n",
    "\n",
    "embeds = load_frcows(\"lemma-A-pos-small.txt\")\n",
    "frcows_examples = [(embeds[lemma], find_class(word)) for lemma, word in examples if lemma in embeds.keys()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading preprocessed fasttext vectors from our examples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "loaded_vectors = KeyedVectors.load('eur_vectors.kv')\n",
    "fasttext_examples = [(loaded_vectors.get_vector(lemma), find_class(get_feminine_word(lemma, examples))) for lemma in loaded_vectors.key_to_index.keys()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split in train/test corpus\n",
    "* thick = frcow 500m\n",
    "* thin = frcow 2.5m\n",
    "* B = frcow 8.8bn\n",
    "* C = fasttext"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split([example[0] for example in own_examples_thick], [example[1] for example in own_examples_thick], test_size=0.2, random_state=42)\n",
    "X_train_A, X_test_A, y_train_A, y_test_A = train_test_split([example[0] for example in own_examples_thin], [example[1] for example in own_examples_thin], test_size=0.2, random_state=42)\n",
    "X_train_B, X_test_B, y_train_B, y_test_B = train_test_split([example[0] for example in frcows_examples], [example[1] for example in frcows_examples], test_size=0.2, random_state=42)\n",
    "X_train_C, X_test_C, y_train_C, y_test_C = train_test_split([example[0] for example in fasttext_examples], [example[1] for example in fasttext_examples], test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Layer Perceptron accuracy on clf_A(frcow 500m): 0.8379166666666664\n",
      "Multi-Layer Perceptron accuracy on clf_A(frcow 2.5m): 0.6625000000000003\n",
      "Multi-Layer Perceptron accuracy on clf_B(frcow 8.8bn): 0.8933333333333324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garri\\Desktop\\Cours\\morphology\\morpho-classifier\\M1_ML2_TD8_pytorch_embeddings\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garri\\Desktop\\Cours\\morphology\\morpho-classifier\\M1_ML2_TD8_pytorch_embeddings\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garri\\Desktop\\Cours\\morphology\\morpho-classifier\\M1_ML2_TD8_pytorch_embeddings\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garri\\Desktop\\Cours\\morphology\\morpho-classifier\\M1_ML2_TD8_pytorch_embeddings\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garri\\Desktop\\Cours\\morphology\\morpho-classifier\\M1_ML2_TD8_pytorch_embeddings\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garri\\Desktop\\Cours\\morphology\\morpho-classifier\\M1_ML2_TD8_pytorch_embeddings\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garri\\Desktop\\Cours\\morphology\\morpho-classifier\\M1_ML2_TD8_pytorch_embeddings\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Layer Perceptron accuracy on clf_C(fasttext): 0.92\n"
     ]
    }
   ],
   "source": [
    "#frcow 500m\n",
    "clf_score=0\n",
    "for count in range(100):\n",
    "    clf = MLPClassifier(max_iter=500)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    clf_score += sklearn.metrics.accuracy_score(pred, y_test)\n",
    "print(\"Multi-Layer Perceptron accuracy on clf(frcow 500m):\",clf_score/100)\n",
    "\n",
    "#frcow 2.5m\n",
    "clf_A_score=0\n",
    "for count in range(100):\n",
    "    clf_A = MLPClassifier(max_iter=500)\n",
    "    clf_A.fit(X_train_A, y_train_A)\n",
    "    pred_A = clf_A.predict(X_test_A)\n",
    "    clf_A_score += sklearn.metrics.accuracy_score(pred_A, y_test_A)\n",
    "print(\"Multi-Layer Perceptron accuracy on clf_A(frcow 2.5m):\",clf_A_score/100)\n",
    "\n",
    "\n",
    "#frcow 8.8bn\n",
    "clf_B_score=0\n",
    "for count in range(100):\n",
    "    clf_B = MLPClassifier(max_iter=500)\n",
    "    clf_B.fit(X_train_B, y_train_B)\n",
    "    pred_B = clf_B.predict(X_test_B)\n",
    "    clf_B_score += sklearn.metrics.accuracy_score(pred_B, y_test_B)\n",
    "print(\"Multi-Layer Perceptron accuracy on clf_B(frcow 8.8bn):\",clf_B_score/100)\n",
    "\n",
    "#fasttext\n",
    "clf_C_score = 0\n",
    "for count in range(100):\n",
    "    clf_C = MLPClassifier(max_iter=500)\n",
    "    clf_C.fit(X_train_C, y_train_C)\n",
    "    pred_C = clf_C.predict(X_test_C)\n",
    "    clf_C_score += sklearn.metrics.accuracy_score(pred_C, y_test_C)\n",
    "print(\"Multi-Layer Perceptron accuracy on clf_C(fasttext):\",clf_C_score/100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7872168284789643\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for example in examples:\n",
    "    if find_class(example[1]) == 1:\n",
    "        counter += 1\n",
    "print(counter/len(examples))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron accuracy on clf_A(frcow 500m): 0.796875\n",
      "Perceptron accuracy on clf_A(frcow 2.5m): 0.6071428571428559\n",
      "Perceptron accuracy on clf_B(frcow 8.8bn): 0.8477366255144019\n",
      "Perceptron accuracy on clf_C(fasttext): 0.9203980099502477\n"
     ]
    }
   ],
   "source": [
    "#frcow 500m\n",
    "clf_score = 0\n",
    "for count in range(100):\n",
    "    clf = Perceptron(max_iter=500)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    clf_score += sklearn.metrics.accuracy_score(pred, y_test)\n",
    "print(\"Perceptron accuracy on clf_A(frcow 500m):\", clf_score/100)\n",
    "\n",
    "#frcow 2.5m\n",
    "clf_A_score = 0\n",
    "for count in range(100):\n",
    "    clf_A = Perceptron(max_iter=500)\n",
    "    clf_A.fit(X_train_A, y_train_A)\n",
    "    pred_A = clf_A.predict(X_test_A)\n",
    "    clf_A_score += sklearn.metrics.accuracy_score(pred_A, y_test_A)\n",
    "print(\"Perceptron accuracy on clf_A(frcow 2.5m):\",clf_A_score/100)\n",
    "\n",
    "#frcow 8.8bn\n",
    "clf_B_score = 0\n",
    "for count in range(100):\n",
    "    clf_B = Perceptron(max_iter=500)\n",
    "    clf_B.fit(X_train_B, y_train_B)\n",
    "    pred_B = clf_B.predict(X_test_B)\n",
    "    clf_B_score += sklearn.metrics.accuracy_score(pred_B, y_test_B)\n",
    "print(\"Perceptron accuracy on clf_B(frcow 8.8bn):\",clf_B_score/100)\n",
    "\n",
    "#fasttext\n",
    "clf_C_score = 0\n",
    "for count in range(100):\n",
    "    clf_C = Perceptron(max_iter=500)\n",
    "    clf_C.fit(X_train_C, y_train_C)\n",
    "    clf_C_score += sklearn.metrics.accuracy_score(pred_C, y_test_C)\n",
    "print(\"Perceptron accuracy on clf_C(fasttext):\",clf_C_score/100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "a = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}